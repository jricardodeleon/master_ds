---
title: "House Pricing Prediction 'AMES' Dataset"
author: "Ricardo De Le칩n Flores."
date: "Date: `r format(Sys.Date(), '%d de %B de %Y')`"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
# install.packages("tidymodels")
library(tidymodels)
# install.packages("mdsr") 
# install.packages("ggpubr")
# install.packages("tictoc")
library(mdsr)
library(ggplot2)
library(tictoc)
library(ggpubr)
```
# Introduction

The Ames Housing dataset is a popular dataset in the field of predictive modeling and machine learning. It contains information about various attributes of residential homes in Ames, Iowa, USA. This dataset is commonly used for tasks such as regression analysis and predictive modeling to predict house prices based on different features.

## Dataset Description:

The dataset comprises **2930** observations and **74** variables, covering a wide range of attributes related to residential properties. Some of the key variables include:

- **Sale_Price**: The sale price of the property.
- **Year_Built**: The year when the property was built.
- **Overall_Qual**: Overall material and finish quality of the house.
- **Overall_Cond**: Overall condition rating of the house.
- **Lot_Area**: Lot size in square feet.
- **Total_Bsmt_SF**: Total basement area in square feet.
- **Bedroom_AbvGr**: Number of bedrooms above ground.
- **Garage_Area**: Garage area in square feet.
- **Neighborhood**: Physical locations within Ames city limits.

This dataset is often used for tasks such as predicting house prices based on various features like size, location, condition, etc.

The **question** worked in this short book is about the increment of house sales through the years.

# Analysis

Load the datase ames

```{r}
data(ames)
head(ames)
```

Lest create a new working copy from ames main dataset

```{r}
ames_df <- data.frame(ames)
colSums(is.na(ames_df))
```

**Bars** graph as firs approach analysis visualization about the prices, with this graph we can se that there is a considerable variety of sales.

```{r}
ames_df %>% 
  ggplot(aes(Sale_Price, ..density..)) + 
  geom_histogram(bins = 30) + 
  geom_density() + 
  labs(x = "Sales", y = "Density") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5)) +
  ggtitle("Sales Price")


```

```{r}
# Determine the minimum and maximum year
min_year <- min(ames_df$Year_Built)
max_year <- max(ames_df$Year_Built)

# Create breaks every 20 years from min to max
breaks <- c(seq(min_year, max_year, by = 20), max_year)

ggplot(ames_df, aes(x = factor(Year_Built))) + 
  geom_bar() +
  labs(x = "Timeline", y = "# Properties") +
  scale_x_discrete(breaks = breaks, labels = breaks) +
  theme(axis.text.x = element_text(hjust = 1),
        plot.title = element_text(hjust = .5)) +
  ggtitle("Properties by Year Built")
```

The graph above gives and idea about the behavior of the sales trough the years, we can see that there are many peaks in different times we could research looking forward in other analysis. 

Now, if would like to know what neighborhood has better sales we can create a graph to show this relation between **Sales_Price** and **Neighborhood** variables
```{r}
# Count the number of sales by neighborhood
neighborhood_counts <- table(ames_df$Neighborhood)

# Convert the counts to a data frame
neighborhood_counts_df <- as.data.frame(neighborhood_counts)
names(neighborhood_counts_df) <- c("Neighborhood", "Sales_Count")

# Sort the data frame by sales count
neighborhood_counts_df <- neighborhood_counts_df[order(-neighborhood_counts_df$Sales_Count),]

# Bar plot
ggplot(neighborhood_counts_df, aes(x = reorder(Neighborhood, Sales_Count), y = Sales_Count)) +
  geom_bar(stat = "identity") +
  labs(x = "Neighborhood", y = "Number of Sales") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(hjust = .5)) +
  ggtitle("Number of Sales by Neighborhood (Bar Plot)")
```

This last graph graph shows the variation of prices on each neighborhood

```{r}
ggplot(ames_df, aes(x=Neighborhood,y=Sale_Price)) + geom_boxplot(color="black", fill="orange")+
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+
scale_y_continuous(labels = comma)
```

# Reression

## Model Decision Tree

### Split train and test

First step is to split into **train** and **test** the dataset, using a seed to work with the same results always
```{r}
set.seed(123)
split_initial <- initial_split(data = ames_df,
              prop = 0.8, # training proportion
              strata = Sale_Price, # Variable objetivo
              # hace el split lo m치s distribuido del precio automaticamente
              )

data_train <- training(split_initial)
data_test <- testing(split_initial)
```

### Recipe

Next step is to apply the **recipe** and create a transformation, should be applied to both, train and test datasets.

```{r echo=T, results='hide'}

transformer <- recipe(formula = Sale_Price ~.,
        data = data_train) |>
        step_naomit(all_predictors()) |> 
        step_nzv(all_predictors()) |> 
        step_center(all_numeric(), -all_outcomes()) |> 
        step_scale(all_numeric(), -all_outcomes()) |> 
        step_dummy(all_nominal(), -all_outcomes()) 

transformer_fit <- prep(transformer)

data_train_prep <- bake(transformer_fit, new_data = data_train)
data_test_prep <- bake(transformer_fit, new_data = data_test)
```

### H-parameters and C-Validation

Once we have the recipe, continue with the model, hyper-parameters and cross-validation

```{r}
#Define the hyper parameter to optimize

model_tree <- decision_tree(mode = "regression", #regression, classification
              tree_depth = tune(),
              min_n = tune()) %>% 
      set_engine(engine = "rpart")

cv_folds <- vfold_cv(data = data_train,
         v = 5, #numero de particiones
         strata = Sale_Price #variable objetivo
         )
```

### Grid Search

Selected **Grid regular** as the grid option to select the hyper-parameters.

```{r}

tic("Decision tree time")
#Grid search (Example)
hiperpar_grid <- grid_regular(
  #Rango de busqueda de cada hiperparametro
  tree_depth(range = c(1,10),trans = NULL),
  min_n( range = c(2,50),trans = NULL ),
  #numero de valores para cada hiperametro
  levels = c(10,10)
)

#Optimization
grid_fit <- tune_grid(
  object = model_tree,
  #Pre procesamiento de los datos
  #Recipe
  preprocessor = transformer,
  #Resample que se crearon sin preprocesar
  resamples = cv_folds,
  #Metricas evaluacion
  metrics = metric_set(rmse,mae),
  control = control_grid(save_pred = TRUE),
  grid = hiperpar_grid
)
time_decision_tree_model <- toc()
```

### Evaluation of training error

```{r echo=T, results='hide'}
grid_fit

grid_fit %>% unnest(.metrics)

grid_fit %>% collect_metrics(summarize = TRUE)

grid_fit %>% show_best(metric = "rmse",n=5)

grid_fit %>% show_best(metric = "mae",n=5)

p1 <- grid_fit %>% collect_metrics(summarize = FALSE) %>% 
  ggplot(aes(x=.metric,y=.estimate,fill=.metric,color=.metric)) +
  geom_boxplot() +
  coord_flip()

p2 <-  grid_fit %>% collect_metrics(summarize = FALSE) %>% 
  ggplot(aes(x=.estimate,fill=.metric)) +
  geom_density(alpha=0.5) 

ggarrange(p1,p2,nrow=2)
```
### Final Model

Select the best model according with the hyper-parameters.

```{r}
best_hip <- select_best(grid_fit,metric="rmse")

final_model_tree <- finalize_model(x=model_tree,
                                   parameters = best_hip )
final_model_tree_fit <- final_model_tree %>% 
  fit(formula = Sale_Price ~.,
      data = data_train_prep)
```

### Model Validation

```{r}
predicciones <- final_model_tree_fit %>% 
  predict(new_data = data_test_prep,
          type = "numeric")

predicciones %>%  head()
    
predicciones <- predicciones %>% bind_cols(data_test_prep %>% select(Sale_Price))  

rmse(predicciones,truth = Sale_Price, estimate= .pred)
```
### Residuals & Predictions

```{r}
p1 <- predicciones %>%  ggplot(
  aes(x = Sale_Price, y = .pred)
) +
  geom_point(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0, color = "firebrick") +
  labs(title = "Valor predicho vs valor real") +
  theme_bw()

p2 <- predicciones %>% ggplot(
  aes(x = Sale_Price - .pred)
) +
  geom_density() + 
  labs(title = "Distribuci칩n residuos del modelo") +
  theme_bw()

ggarrange(plotlist = list(p1, p2)) %>%
  annotate_figure(
    top = text_grob("Distribuci칩n residuos", size = 15, face = "bold")
  )
```

## Model Random Forest

### Split train and test

```{r}
# Define the hyper parameters to optimize
model_rf <- rand_forest(mode = "regression",
                        mtry = tune(), # Number of variables to sample as candidates at each split
                        trees = tune()) %>% 
  set_engine("ranger")

# Cross Validation
cv_folds_rf <- vfold_cv(data = data_train,
                     v = 5, # Number of folds
                     strata = Sale_Price # Target variable
)
```

### Grid Search

Selected **Grid regular** as the grid option to select the hyper-parameters.

```{r echo=FALSE }
# install.packages("ranger")
library(ranger)

tic("Random forest time")
#Grid search (Example)
hiperpar_grid_rf <- grid_regular(
  mtry(range = c(2, ncol(data_train) - 1)),
  trees(c(500, 1000)) # Number of trees
)

# Define the workflow
rf_wf <- workflow() %>%
  add_model(model_rf) %>%
  add_formula(Sale_Price ~ .) # Assuming 'Sale_Price' is the target variable

# Tune the model
grid_fit_rf <- tune_grid(
  object = rf_wf,
  resamples = cv_folds_rf,
  grid = hiperpar_grid_rf,
  metrics = metric_set(rmse),
  control = control_grid(verbose = TRUE)
)
time_random_forest_model <- toc()
```

### Evaluation of training error

```{r}
grid_fit_rf

grid_fit_rf %>% unnest(.metrics)

grid_fit_rf %>% collect_metrics(summarize = TRUE)

grid_fit_rf %>% show_best(metric = "rmse",n=5)

p1_rf <- grid_fit_rf %>% collect_metrics(summarize = FALSE) %>% 
  ggplot(aes(x=.metric,y=.estimate,fill=.metric,color=.metric)) +
  geom_boxplot() +
  coord_flip()

p2_rf <-  grid_fit_rf %>% collect_metrics(summarize = FALSE) %>% 
  ggplot(aes(x=.estimate,fill=.metric)) +
  geom_density(alpha=0.5) 

ggarrange(p1_rf,p2_rf,nrow=2)
```

### Final Model

Select the best model according with the hyper-parameters.

```{r}
best_hip_rf <- select_best(grid_fit_rf,metric="rmse")

final_model_tree_rf <- finalize_model(x=model_rf,
                                   parameters = best_hip_rf )
final_model_tree_fit_rf <- final_model_tree_rf %>% 
  fit(formula = Sale_Price ~.,
      data = data_train_prep)
```

### Model Validation

```{r}
predicciones_rf <- final_model_tree_fit_rf %>% 
  predict(new_data = data_test_prep,
          type = "numeric")

predicciones_rf %>%  head()
    
predicciones_rf <- predicciones_rf %>% bind_cols(data_test_prep %>% select(Sale_Price))  

rmse(predicciones_rf,truth = Sale_Price, estimate= .pred)
```


### Residuals & Predictions


```{r}

p1_rf <- predicciones_rf %>%  ggplot(
  aes(x = Sale_Price, y = .pred)
) +
  geom_point(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0, color = "firebrick") +
  labs(title = "Valor predicho vs valor real") +
  theme_bw()

p2_rf <- predicciones_rf %>% ggplot(
  aes(x = Sale_Price - .pred)
) +
  geom_density() + 
  labs(title = "Distribuci칩n residuos del modelo") +
  theme_bw()

ggarrange(plotlist = list(p1, p2)) %>%
  annotate_figure(
    top = text_grob("Distribuci칩n residuos", size = 15, face = "bold")
  )
```



# Conclusions:

In Regression models the metrics shows a better result with the model **Random Forest** with an rmse of 29304.98	whereas **Decision Tree** has an rmse of 42820.28. The model should be adjusted better in order to get a lower rmse.




